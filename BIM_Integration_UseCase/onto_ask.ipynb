{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 13:25:30,276 - INFO - Successfully loaded schema ontology\n",
      "2024-12-06 13:25:31,079 - INFO - Successfully loaded data ontology\n",
      "2024-12-06 13:25:32,355 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-06 13:25:32,360 - INFO - Successfully generated SPARQL query\n",
      "2024-12-06 13:25:32,506 - INFO - Successfully executed query. Found 2 results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated SPARQL Query:\n",
      "--------------------------------------------------------------------------------\n",
      "PREFIX : <http://w3id.org/IproK/00/BIMOnto#>\n",
      "SELECT ?storey (COUNT(?window) AS ?windowCount)\n",
      "WHERE {\n",
      "  ?window a :IfcWindow ;\n",
      "         :isLocatedAt ?storey .\n",
      "}\n",
      "GROUP BY ?storey\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-06 13:25:33,597 - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2024-12-06 13:25:33,600 - INFO - Successfully formatted results with LLM\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Formatted Explanation:\n",
      "--------------------------------------------------------------------------------\n",
      "The SPARQL query counted the number of windows on each storey in the building model. The query results show that there are 12 windows located on the \"GF\" storey (Ground Floor) and 9 windows located on the \"01_Floor\" storey (First Floor).\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SPARQLQueryGenerator:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        self.client = OpenAI()\n",
    "        \n",
    "    def _prepare_prompt(self, question: str, ontology_serialization: str) -> str:\n",
    "        \"\"\"Prepare the prompt for OpenAI.\"\"\"\n",
    "        return f\"\"\"\n",
    "        The following is an ontology graph with classes and properties:\n",
    "        \n",
    "        {ontology_serialization}\n",
    "        \n",
    "        Using this ontology, convert the following natural language question to a SPARQL query:\n",
    "        \n",
    "        Question: \"{question}\"\n",
    "        \n",
    "        Return only the SPARQL query without any additional text or markdown formatting.\n",
    "        \"\"\"\n",
    "    \n",
    "    def generate_sparql_query(self, question: str, schema_graph: Graph) -> str:\n",
    "        \"\"\"Generate a SPARQL query from a natural language question.\"\"\"\n",
    "        try:\n",
    "            # Serialize the ontology schema\n",
    "            ontology_serialization = schema_graph.serialize(format='turtle')\n",
    "            \n",
    "            # Prepare the prompt\n",
    "            prompt = self._prepare_prompt(question, ontology_serialization)\n",
    "            \n",
    "            # Get response from OpenAI\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant for generating SPARQL queries. Return only the SPARQL query without any additional text or markdown formatting.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=1500\n",
    "            )\n",
    "            \n",
    "            # Extract and clean the query\n",
    "            query = response.choices[0].message.content.strip()\n",
    "            cleaned_query = query.replace(\"```sparql\", \"\").replace(\"```\", \"\").strip()\n",
    "            \n",
    "            logger.info(\"Successfully generated SPARQL query\")\n",
    "            return cleaned_query\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating SPARQL query: {str(e)}\")\n",
    "            raise ValueError(f\"Failed to generate SPARQL query: {str(e)}\")\n",
    "\n",
    "    def format_results_with_llm(self, results: list, query: str) -> str:\n",
    "        \"\"\"Format SPARQL query results using OpenAI.\"\"\"\n",
    "        try:\n",
    "            # Prepare prompt for LLM\n",
    "            prompt = f\"\"\"\n",
    "            The following SPARQL query was executed:\n",
    "            \n",
    "            {query}\n",
    "            \n",
    "            The query results are:\n",
    "            {results}\n",
    "            \n",
    "            Please format these results into a human-readable explanation.\n",
    "            \"\"\"\n",
    "            \n",
    "            # Get formatted response from OpenAI\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Your task is to format query results into human-understandable explanations.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                max_tokens=500\n",
    "            )\n",
    "            \n",
    "            formatted_response = response.choices[0].message.content.strip()\n",
    "            logger.info(\"Successfully formatted results with LLM\")\n",
    "            return formatted_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error formatting results with LLM: {str(e)}\")\n",
    "            raise ValueError(f\"Failed to format results: {str(e)}\")\n",
    "\n",
    "    def execute_query(self, query: str, data_graph: Graph) -> Tuple[bool, Optional[str]]:\n",
    "        \"\"\"Execute a SPARQL query, process results with LLM, and return the explanation.\"\"\"\n",
    "        try:\n",
    "            # Execute query on the data graph\n",
    "            results = list(data_graph.query(query))\n",
    "            logger.info(f\"Successfully executed query. Found {len(results)} results.\")\n",
    "            \n",
    "            # Format results with LLM\n",
    "            explanation = self.format_results_with_llm(results, query)\n",
    "            return True, explanation\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error executing SPARQL query: {str(e)}\")\n",
    "            return False, None\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # Initialize the query generator\n",
    "        generator = SPARQLQueryGenerator()\n",
    "        \n",
    "        # Load the ontology schema\n",
    "        schema_graph = Graph()\n",
    "        schema_graph.parse(\"base_bimonto.ttl\", format='turtle')\n",
    "        logger.info(\"Successfully loaded schema ontology\")\n",
    "        \n",
    "        # Load the ontology with data\n",
    "        data_graph = Graph()\n",
    "        data_graph.parse(\"filled_bimonto.ttl\", format='turtle')\n",
    "        logger.info(\"Successfully loaded data ontology\")\n",
    "        \n",
    "        # Example question\n",
    "        question = \"count the number of windows on each building storey\"\n",
    "        \n",
    "        # Generate and execute query\n",
    "        sparql_query = generator.generate_sparql_query(question, schema_graph)\n",
    "        print(\"\\nGenerated SPARQL Query:\")\n",
    "        print(\"-\" * 80)\n",
    "        print(sparql_query)\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Execute the query and get explanation\n",
    "        success, explanation = generator.execute_query(sparql_query, data_graph)\n",
    "        \n",
    "        if success and explanation:\n",
    "            print(\"\\nFormatted Explanation:\")\n",
    "            print(\"-\" * 80)\n",
    "            print(explanation)\n",
    "        else:\n",
    "            print(\"No results found or query execution failed.\")\n",
    "            \n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Failed to load ontology file: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
